{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext, Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp import Finisher\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[0;32m      2\u001b[0m     \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mnlp converter\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39m16G\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      5\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.maxResultSize\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m300\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.kryoserializer.buffer.max\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m2000M\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      7\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.jars.packages\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp:spark-nlp_2.12:4.2.6\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[0;32m      8\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      9\u001b[0m sqlContext \u001b[39m=\u001b[39m SQLContext(spark)\n\u001b[0;32m     10\u001b[0m spark\n",
      "File \u001b[1;32mc:\\Users\\Luisa\\Desktop\\.venv\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    268\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    270\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Luisa\\Desktop\\.venv\\lib\\site-packages\\pyspark\\context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    482\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 483\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    484\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Luisa\\Desktop\\.venv\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Luisa\\Desktop\\.venv\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Luisa\\Desktop\\.venv\\lib\\site-packages\\pyspark\\java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 103\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"nlp converter\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"300\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.6\")\\\n",
    "    .getOrCreate()\n",
    "sqlContext = SQLContext(spark)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"files\\raw_texts.feather\"\n",
    "# pandas_df =  pd.read_feather(source+path).drop_duplicates(subset = 'URL', keep = 'first').reset_index(drop=True)\n",
    "# pandas_df = pandas_df[pandas_df['URL_TEXT']!=\"\"]\n",
    "# spark_df = sqlContext.createDataFrame(pandas_df)\n",
    "\n",
    "source = os.path.dirname(os.path.realpath('__file__')).split(\"src\")[0]\n",
    "path = r\"files\\raw_texts.parquet\"\n",
    "spark_df = spark.read.parquet(source+path).where(f.col(\"URL_TEXT\")!= \"\")\n",
    "spark_df = spark_df.withColumn('URL_TEXT', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = [\"(?:<from.*?>)(.*?)(?:<\\\\/from>)\"]\n",
    "html = [\"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\"]        \n",
    "random_pattern = [r'^@.*\\{.*\\}', r'^\\..*\\{.*\\}',r'\\s\\s+',r'\\n',r'\\xa0',r'dbx707', r'\\xe2',r'\\x80',r\"\\x8b\", r\"{{\\.*}}\", r\"\\x9d\", r\"\\u200b\"]# only digits: r'\\b[0-9]+\\b\\s*'\n",
    "url = [\"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[äöüßa-zA-Z0-9()]{1,6}\\\\b(?:[-a-zäöüßA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\", \"www\\w*de\",\"www\\w*com\"]\n",
    "email = [\"^\\S+@\\S+\\.\\S+$\"]\n",
    "zip = [\"^[0-9]{5}(?:-[0-9]{4})?\\s?\\w*$\"]\n",
    "phone = [\"^\\\\+?[1-9][0-9]{7,14}$\"]\n",
    "dates = [\"^[0-9]{1,2}\\\\/[0-9]{1,2}\\\\/[0-9]{4}$\",\"^[0-9]{1,2}\\\\-[0-9]{1,2}\\\\-[0-9]{4}$\", \"^[0-9]{4}\\\\-[0-9]{1,2}\\\\-[0-9]{1,2}$\"]\n",
    "website_stopwords = [\"explore\",\"allgemeine geschäftsbedingungen\",\"allgemein\\*\",'richtlinie\\w*',\"\\w*recht\\w* hinweis\\w*\",\"\\w*recht\\w*\",\"\\w*datenschutz\\w*\", \"privacy\",\"policy\\w*\",\"cooky\\w*\",\"cookie\\w*\",\"content\\w*\",\" to \",\\\n",
    "        \"anmeld\\w*\",  \"abmeld\\w*\", \"login\",\"log in\",\"logout\", \"log out\", \"kunden login\", \"online\",\"zurück\",\"back\",\"start\",\"select\\w*\", \"ausw\\w*\",\"close\",\\\n",
    "            \"extras\",\"news\",\"report\\w*\",\"impressum\",\"newsletter\\w*\", \"owner\",\"internet\", \"website\\w*\", \"email\\w*\", \"e-mail\\w*\", \"mail\\w*\", \"isbn\", \"issn\",\\\n",
    "                \"produkte\", \"partner\",\"übersicht\", \"veranstaltungen\", \"suche\\w*\",\"kauf\\w*\", \"angebot\\w*\", \"konfigur\\w*\", \"configur\\w*\",\"nutzer\\w*\",\"icon\\w*\",\\\n",
    "                    \"zubehör\", \"garantie\", \"mehr\", \"modell\\w*\", \"kontakt\\w*\",\"contact\\w*\",\"anfrage\\w*\",\"skip\",'useful links','link\\w*',\"pin\\w*\",\"passw\\w*\", \"password\\w*\",\\\n",
    "                        \"buchen\",\"book\" \"anfahrt\", \"finanzdienstleistung\\w*\" \"connected\", \"required\", \"sitemap\\w*\", \"\\w*\\s?abo\\w*\", 'social media', \"socialmedia\",\\\n",
    "                            \"englisch\", \"english\",\"deutsch\",\"german\",\"google\", \"wikipedia\", \"navigation\",\"\\w*shop\\w*\", \"\\w*magazin\\w*\", \"lifestyle\",\\\n",
    "                                \"facebook\\w*\", \"youtube\\w*\",\"instagram\\w*\",\"xing\\w*\",\"linkedin\\w*\", \"blog\\w*\",\"spiegel\\w*\",\"twitter\\w*\",\"sms\",\"video\"\\\n",
    "                                    \"archiv\\w*\", \"artikel\\w*\", \"article\\w*\",\"side\\w*\", \"seite\\w*\",\"site\",\"app\\w*\",\"\\s?abgerufen\\s?\\w*\\s*\\d*\",\\\n",
    "                                        \"januar\", \"februar\", \"märz\", \"april\", \"mai\", \"juni\", \"juli\", \"august\", \"september\", \"oktober\", \"november\", \"dezember\",\\\n",
    "                                            \"dbx707\", \"db11\",\"\\w*\\s?straße\\s?\\d*\",\"\\w*\\s?strasse\\w*\", \"tel\\w*\", \"\\w*\\s?download\\w*\",\\\n",
    "                                                \"covid\\w*\\s?\\d*\", \"corona\\w*\\s?\\d*\"]\n",
    "                                \n",
    "domain_stopwords = [\"(g/km)\",\"use case\\w*\", \"unternehme\\w*\", \"gmbh\", \"cokg\", \"co kg\", \"consult\\w*\", \"handel\\w*\", \"händler\\w*\", \"leistung\\w*\"]\n",
    "numbers_only = [\"^\\\\d+$\",\"^\\s?[0-9]+(\\s+[0-9]+)*\\s?$\", \"\\(.*\\)\",\"\\[.*\\]\", \"^\\d+.\\d+\",\" \\\\d+ \"]\n",
    "special_characters = ['[^äöüßA-Za-z0-9 ]+']#['[\\(,.:\\);^]']\n",
    "short_words = ['^\\w{0,3}$', '^\\s+']\n",
    "\n",
    "all_pattern_to_remove = xml+html+random_pattern+url+email+zip+phone+dates+website_stopwords+domain_stopwords+numbers_only+special_characters\n",
    "spark_df = spark_df.withColumn('text', 'URL_TEXT')\n",
    "for pattern in all_pattern_to_remove:\n",
    "    spark_df = spark_df.withColumn(\"text\", f.regexp_replace('text', pattern, \"\")) \n",
    "spark_df = spark_df.withColumn('text', f.trim(f.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "language_detector = LanguageDetectorDL.pretrained(\"ld_wiki_tatoeba_cnn_21\")\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"lang\")\\\n",
    "    .setThreshold(0.8)\\\n",
    "    .setCoalesceSentences(True)\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"Sentence\")\n",
    "\n",
    "regexTokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setIncludeMetadata(True)\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    ".setStages([\n",
    "    documentAssembler,\n",
    "    language_detector,\n",
    "    sentenceDetector,\n",
    "    regexTokenizer,\n",
    "    finisher\n",
    "    ])\n",
    "\n",
    "result = pipeline.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29e3f5a20b6bf6aef5494d745dbb8622970e33bcbdb37d238be21287c6d6aad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
